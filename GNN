{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1755671113853,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"sD8fkgZiyd6G"},"outputs":[],"source":["!gunzip /content/email-Eu-core.txt.gz\n","!gunzip /content/email-Eu-core-department-labels.txt.gz"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28508,"status":"ok","timestamp":1755671147310,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"ulaQokXHvgYK","outputId":"64f6e91a-4ffd-42d1-bf35-ecdf3bf1175b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_geometric\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.12.15)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.8.3)\n","Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_geometric\n","Successfully installed torch_geometric-2.6.1\n"]}],"source":["!pip install torch_geometric\n","import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCbZa_f7gJgG","outputId":"b8c37e49-07c4-46e7-dd27-09a5b42aa0f9"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","INFO:root:\n","Unfortunately, your original traceback can not be constructed.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","INFO:root:\n","Unfortunately, your original traceback can not be constructed.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","INFO:root:\n","Unfortunately, your original traceback can not be constructed.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-1901043294.py\", line 30, in <cell line: 0>\n","    betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/utils/decorators.py\", line 845, in func\n","    return argmap._lazy_compile(__wrapper)(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<class 'networkx.utils.decorators.argmap'> compilation 12\", line 4, in argmap_betweenness_centrality_9\n","    import inspect\n","            ^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line 130, in betweenness_centrality\n","    S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line None, in _single_source_shortest_path_basic\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n","    start = lineno - 1 - context//2\n","            ~~~~~~~^~~\n","TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-1901043294.py\", line 30, in <cell line: 0>\n","    betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/utils/decorators.py\", line 845, in func\n","    return argmap._lazy_compile(__wrapper)(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<class 'networkx.utils.decorators.argmap'> compilation 12\", line 4, in argmap_betweenness_centrality_9\n","    import inspect\n","            ^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line 130, in betweenness_centrality\n","    S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line None, in _single_source_shortest_path_basic\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","           ^^^^^^^^^^^^\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n","    start = lineno - 1 - context//2\n","            ~~~~~~~^~~\n","TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-1901043294.py\", line 30, in <cell line: 0>\n","    betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/utils/decorators.py\", line 845, in func\n","    return argmap._lazy_compile(__wrapper)(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<class 'networkx.utils.decorators.argmap'> compilation 12\", line 4, in argmap_betweenness_centrality_9\n","    import inspect\n","            ^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line 130, in betweenness_centrality\n","    S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/networkx/algorithms/centrality/betweenness.py\", line None, in _single_source_shortest_path_basic\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","           ^^^^^^^^^^^^\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","           ^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","           ^^^^^^^^^^^^\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n","    start = lineno - 1 - context//2\n","            ~~~~~~~^~~\n","TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n"]}],"source":["#Training Graph SAGE Model\n","import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/weighted_directed_graph.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26717,"status":"ok","timestamp":1755135518140,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"4Tt8UwhN1mMz","outputId":"5d4e0ce9-02f8-40e3-fb1c-63c0e86b16fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 1.0093, Train Acc: 0.6169, Val Loss: 1.2567, Val Acc: 0.3571\n","Epoch 0040, Train Loss: 0.6928, Train Acc: 0.7460, Val Loss: 1.2285, Val Acc: 0.5714\n","Epoch 0060, Train Loss: 0.5557, Train Acc: 0.7782, Val Loss: 1.2694, Val Acc: 0.4643\n","Epoch 0080, Train Loss: 0.5049, Train Acc: 0.7984, Val Loss: 1.3392, Val Acc: 0.5714\n","Epoch 0100, Train Loss: 0.4029, Train Acc: 0.8387, Val Loss: 1.1895, Val Acc: 0.6786\n","Epoch 0120, Train Loss: 0.4014, Train Acc: 0.8589, Val Loss: 1.4262, Val Acc: 0.6429\n","Epoch 0140, Train Loss: 0.2874, Train Acc: 0.8952, Val Loss: 1.4347, Val Acc: 0.6429\n","Epoch 0160, Train Loss: 0.3153, Train Acc: 0.8669, Val Loss: 1.4753, Val Acc: 0.6786\n","Epoch 0180, Train Loss: 0.2630, Train Acc: 0.9153, Val Loss: 1.6397, Val Acc: 0.6429\n","Epoch 0200, Train Loss: 0.2312, Train Acc: 0.8992, Val Loss: 1.4669, Val Acc: 0.6786\n","Epoch 0220, Train Loss: 0.1980, Train Acc: 0.9395, Val Loss: 1.5286, Val Acc: 0.6429\n","Epoch 0240, Train Loss: 0.1680, Train Acc: 0.9395, Val Loss: 1.4962, Val Acc: 0.7500\n","Epoch 0260, Train Loss: 0.2015, Train Acc: 0.9274, Val Loss: 1.7285, Val Acc: 0.6429\n","Epoch 0280, Train Loss: 0.1312, Train Acc: 0.9597, Val Loss: 1.5823, Val Acc: 0.7143\n","Epoch 0300, Train Loss: 0.1408, Train Acc: 0.9476, Val Loss: 1.4413, Val Acc: 0.6786\n","Epoch 0320, Train Loss: 0.2143, Train Acc: 0.9234, Val Loss: 1.7010, Val Acc: 0.6786\n","Epoch 0340, Train Loss: 0.1543, Train Acc: 0.9395, Val Loss: 1.3976, Val Acc: 0.6429\n","Epoch 0360, Train Loss: 0.1264, Train Acc: 0.9677, Val Loss: 1.4854, Val Acc: 0.7143\n","Epoch 0380, Train Loss: 0.1311, Train Acc: 0.9556, Val Loss: 1.4507, Val Acc: 0.7143\n","Epoch 0400, Train Loss: 0.1308, Train Acc: 0.9556, Val Loss: 1.4232, Val Acc: 0.6786\n","Epoch 0420, Train Loss: 0.0978, Train Acc: 0.9597, Val Loss: 1.3592, Val Acc: 0.6786\n","Epoch 0440, Train Loss: 0.1246, Train Acc: 0.9556, Val Loss: 1.5719, Val Acc: 0.6429\n","Epoch 0460, Train Loss: 0.0942, Train Acc: 0.9718, Val Loss: 1.6227, Val Acc: 0.7143\n","Epoch 0480, Train Loss: 0.1187, Train Acc: 0.9597, Val Loss: 1.5086, Val Acc: 0.7500\n","Epoch 0500, Train Loss: 0.0869, Train Acc: 0.9677, Val Loss: 1.5032, Val Acc: 0.7143\n","Epoch 0520, Train Loss: 0.0929, Train Acc: 0.9839, Val Loss: 1.4691, Val Acc: 0.7143\n","Epoch 0540, Train Loss: 0.1183, Train Acc: 0.9597, Val Loss: 1.5088, Val Acc: 0.7143\n","Epoch 0560, Train Loss: 0.0878, Train Acc: 0.9597, Val Loss: 1.5813, Val Acc: 0.7143\n","Epoch 0580, Train Loss: 0.0722, Train Acc: 0.9839, Val Loss: 1.6284, Val Acc: 0.7143\n","Epoch 0600, Train Loss: 0.1104, Train Acc: 0.9677, Val Loss: 1.5959, Val Acc: 0.7143\n","Epoch 0620, Train Loss: 0.0928, Train Acc: 0.9677, Val Loss: 1.5048, Val Acc: 0.6786\n","Epoch 0640, Train Loss: 0.0978, Train Acc: 0.9516, Val Loss: 1.5744, Val Acc: 0.7143\n","Epoch 0660, Train Loss: 0.0747, Train Acc: 0.9677, Val Loss: 1.4646, Val Acc: 0.7500\n","Epoch 0680, Train Loss: 0.0768, Train Acc: 0.9839, Val Loss: 1.6701, Val Acc: 0.6786\n","Epoch 0700, Train Loss: 0.1020, Train Acc: 0.9758, Val Loss: 1.5583, Val Acc: 0.7143\n","Epoch 0720, Train Loss: 0.0851, Train Acc: 0.9798, Val Loss: 1.5411, Val Acc: 0.6786\n","Epoch 0740, Train Loss: 0.0786, Train Acc: 0.9718, Val Loss: 1.6608, Val Acc: 0.6786\n","Epoch 0760, Train Loss: 0.0799, Train Acc: 0.9677, Val Loss: 1.5188, Val Acc: 0.6786\n","Epoch 0780, Train Loss: 0.0947, Train Acc: 0.9556, Val Loss: 1.6590, Val Acc: 0.6786\n","Epoch 0800, Train Loss: 0.0915, Train Acc: 0.9677, Val Loss: 1.5625, Val Acc: 0.6786\n","Epoch 0820, Train Loss: 0.0659, Train Acc: 0.9839, Val Loss: 1.7287, Val Acc: 0.6429\n","Epoch 0840, Train Loss: 0.0581, Train Acc: 0.9798, Val Loss: 1.5102, Val Acc: 0.7143\n","Epoch 0860, Train Loss: 0.0688, Train Acc: 0.9758, Val Loss: 1.5799, Val Acc: 0.7143\n","Epoch 0880, Train Loss: 0.0941, Train Acc: 0.9798, Val Loss: 1.6152, Val Acc: 0.6786\n","Epoch 0900, Train Loss: 0.0860, Train Acc: 0.9677, Val Loss: 1.6421, Val Acc: 0.6786\n","Epoch 0920, Train Loss: 0.0780, Train Acc: 0.9798, Val Loss: 1.5101, Val Acc: 0.7143\n","Epoch 0940, Train Loss: 0.0712, Train Acc: 0.9879, Val Loss: 1.5535, Val Acc: 0.6786\n","Epoch 0960, Train Loss: 0.0611, Train Acc: 0.9758, Val Loss: 1.6730, Val Acc: 0.6786\n","Epoch 0980, Train Loss: 0.0548, Train Acc: 0.9839, Val Loss: 1.7554, Val Acc: 0.6786\n","Epoch 1000, Train Loss: 0.0819, Train Acc: 0.9718, Val Loss: 1.5015, Val Acc: 0.7143\n","Test Loss: 1.0247, Test Accuracy: 0.7000\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/doubly_stochastic.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41349,"status":"ok","timestamp":1755135609645,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"-ALY5FfJ9vsS","outputId":"7ce44527-3913-438d-d86e-f8f08db78674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 0.8728, Train Acc: 0.6929, Val Loss: 1.1764, Val Acc: 0.4074\n","Epoch 0040, Train Loss: 0.6521, Train Acc: 0.7593, Val Loss: 0.7252, Val Acc: 0.7407\n","Epoch 0060, Train Loss: 0.5805, Train Acc: 0.7552, Val Loss: 0.6661, Val Acc: 0.7037\n","Epoch 0080, Train Loss: 0.3987, Train Acc: 0.8299, Val Loss: 0.5899, Val Acc: 0.7778\n","Epoch 0100, Train Loss: 0.3189, Train Acc: 0.8921, Val Loss: 0.4384, Val Acc: 0.7778\n","Epoch 0120, Train Loss: 0.3001, Train Acc: 0.8963, Val Loss: 0.4660, Val Acc: 0.7407\n","Epoch 0140, Train Loss: 0.3001, Train Acc: 0.8838, Val Loss: 0.4322, Val Acc: 0.8519\n","Epoch 0160, Train Loss: 0.2236, Train Acc: 0.9087, Val Loss: 0.4400, Val Acc: 0.7407\n","Epoch 0180, Train Loss: 0.1985, Train Acc: 0.9212, Val Loss: 0.3942, Val Acc: 0.8148\n","Epoch 0200, Train Loss: 0.1896, Train Acc: 0.9461, Val Loss: 0.3956, Val Acc: 0.7778\n","Epoch 0220, Train Loss: 0.1597, Train Acc: 0.9461, Val Loss: 0.5292, Val Acc: 0.8519\n","Epoch 0240, Train Loss: 0.1953, Train Acc: 0.9253, Val Loss: 0.4572, Val Acc: 0.7407\n","Epoch 0260, Train Loss: 0.2081, Train Acc: 0.9461, Val Loss: 0.2994, Val Acc: 0.8519\n","Epoch 0280, Train Loss: 0.2063, Train Acc: 0.9336, Val Loss: 0.3849, Val Acc: 0.7778\n","Epoch 0300, Train Loss: 0.1387, Train Acc: 0.9585, Val Loss: 0.4019, Val Acc: 0.8889\n","Epoch 0320, Train Loss: 0.1081, Train Acc: 0.9544, Val Loss: 0.3606, Val Acc: 0.8148\n","Epoch 0340, Train Loss: 0.1588, Train Acc: 0.9336, Val Loss: 0.3693, Val Acc: 0.8889\n","Epoch 0360, Train Loss: 0.0890, Train Acc: 0.9751, Val Loss: 0.3739, Val Acc: 0.8889\n","Epoch 0380, Train Loss: 0.1482, Train Acc: 0.9461, Val Loss: 0.4348, Val Acc: 0.8148\n","Epoch 0400, Train Loss: 0.1308, Train Acc: 0.9419, Val Loss: 0.3850, Val Acc: 0.8889\n","Epoch 0420, Train Loss: 0.1037, Train Acc: 0.9627, Val Loss: 0.2992, Val Acc: 0.8889\n","Epoch 0440, Train Loss: 0.0978, Train Acc: 0.9627, Val Loss: 0.3034, Val Acc: 0.8148\n","Epoch 0460, Train Loss: 0.1435, Train Acc: 0.9419, Val Loss: 0.3244, Val Acc: 0.8519\n","Epoch 0480, Train Loss: 0.0978, Train Acc: 0.9710, Val Loss: 0.5601, Val Acc: 0.7778\n","Epoch 0500, Train Loss: 0.1266, Train Acc: 0.9627, Val Loss: 0.3337, Val Acc: 0.8519\n","Epoch 0520, Train Loss: 0.0910, Train Acc: 0.9668, Val Loss: 0.4194, Val Acc: 0.8519\n","Epoch 0540, Train Loss: 0.0787, Train Acc: 0.9751, Val Loss: 0.3588, Val Acc: 0.8889\n","Epoch 0560, Train Loss: 0.0806, Train Acc: 0.9751, Val Loss: 0.4486, Val Acc: 0.8148\n","Epoch 0580, Train Loss: 0.0792, Train Acc: 0.9710, Val Loss: 0.4132, Val Acc: 0.8519\n","Epoch 0600, Train Loss: 0.0901, Train Acc: 0.9502, Val Loss: 0.3927, Val Acc: 0.8889\n","Epoch 0620, Train Loss: 0.1010, Train Acc: 0.9585, Val Loss: 0.3559, Val Acc: 0.8889\n","Epoch 0640, Train Loss: 0.0763, Train Acc: 0.9668, Val Loss: 0.3129, Val Acc: 0.8148\n","Epoch 0660, Train Loss: 0.0971, Train Acc: 0.9710, Val Loss: 0.4189, Val Acc: 0.7778\n","Epoch 0680, Train Loss: 0.0609, Train Acc: 0.9917, Val Loss: 0.4999, Val Acc: 0.7407\n","Epoch 0700, Train Loss: 0.0948, Train Acc: 0.9876, Val Loss: 0.2975, Val Acc: 0.8889\n","Epoch 0720, Train Loss: 0.0697, Train Acc: 0.9627, Val Loss: 0.4961, Val Acc: 0.8519\n","Epoch 0740, Train Loss: 0.0931, Train Acc: 0.9627, Val Loss: 0.5903, Val Acc: 0.7778\n","Epoch 0760, Train Loss: 0.1045, Train Acc: 0.9710, Val Loss: 0.5320, Val Acc: 0.7778\n","Epoch 0780, Train Loss: 0.0994, Train Acc: 0.9668, Val Loss: 0.3673, Val Acc: 0.8519\n","Epoch 0800, Train Loss: 0.0970, Train Acc: 0.9751, Val Loss: 0.3979, Val Acc: 0.8889\n","Epoch 0820, Train Loss: 0.0791, Train Acc: 0.9710, Val Loss: 0.3868, Val Acc: 0.8519\n","Epoch 0840, Train Loss: 0.0604, Train Acc: 0.9834, Val Loss: 0.3957, Val Acc: 0.9259\n","Epoch 0860, Train Loss: 0.0570, Train Acc: 0.9834, Val Loss: 0.4536, Val Acc: 0.8148\n","Epoch 0880, Train Loss: 0.0740, Train Acc: 0.9834, Val Loss: 0.5431, Val Acc: 0.8148\n","Epoch 0900, Train Loss: 0.0762, Train Acc: 0.9627, Val Loss: 0.3160, Val Acc: 0.8889\n","Epoch 0920, Train Loss: 0.0295, Train Acc: 0.9959, Val Loss: 0.3399, Val Acc: 0.8889\n","Epoch 0940, Train Loss: 0.0617, Train Acc: 0.9751, Val Loss: 0.4717, Val Acc: 0.7407\n","Epoch 0960, Train Loss: 0.0576, Train Acc: 0.9876, Val Loss: 0.3416, Val Acc: 0.8148\n","Epoch 0980, Train Loss: 0.0633, Train Acc: 0.9751, Val Loss: 0.4708, Val Acc: 0.7778\n","Epoch 1000, Train Loss: 0.0523, Train Acc: 0.9834, Val Loss: 0.3907, Val Acc: 0.8889\n","Test Loss: 0.4355, Test Accuracy: 0.8806\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/disparity_filter.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23869,"status":"ok","timestamp":1755135649363,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"H6ljcqLX-H-5","outputId":"1bba59d9-917f-4883-8841-676254d3cda4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 1.1110, Train Acc: 0.5564, Val Loss: 1.2252, Val Acc: 0.4667\n","Epoch 0040, Train Loss: 0.9268, Train Acc: 0.6692, Val Loss: 1.1038, Val Acc: 0.5333\n","Epoch 0060, Train Loss: 0.9081, Train Acc: 0.6541, Val Loss: 1.0731, Val Acc: 0.5333\n","Epoch 0080, Train Loss: 0.7651, Train Acc: 0.7180, Val Loss: 1.0019, Val Acc: 0.5667\n","Epoch 0100, Train Loss: 0.8179, Train Acc: 0.7256, Val Loss: 0.9211, Val Acc: 0.6667\n","Epoch 0120, Train Loss: 0.7416, Train Acc: 0.7444, Val Loss: 0.8759, Val Acc: 0.6667\n","Epoch 0140, Train Loss: 0.6566, Train Acc: 0.7782, Val Loss: 0.8364, Val Acc: 0.6667\n","Epoch 0160, Train Loss: 0.6813, Train Acc: 0.7481, Val Loss: 0.7939, Val Acc: 0.6667\n","Epoch 0180, Train Loss: 0.5938, Train Acc: 0.7782, Val Loss: 0.7759, Val Acc: 0.7000\n","Epoch 0200, Train Loss: 0.5710, Train Acc: 0.7857, Val Loss: 0.7293, Val Acc: 0.7000\n","Epoch 0220, Train Loss: 0.5887, Train Acc: 0.7782, Val Loss: 0.7657, Val Acc: 0.7000\n","Epoch 0240, Train Loss: 0.5460, Train Acc: 0.8008, Val Loss: 0.7028, Val Acc: 0.7000\n","Epoch 0260, Train Loss: 0.4665, Train Acc: 0.8271, Val Loss: 0.6874, Val Acc: 0.7000\n","Epoch 0280, Train Loss: 0.4562, Train Acc: 0.8383, Val Loss: 0.6914, Val Acc: 0.7000\n","Epoch 0300, Train Loss: 0.4765, Train Acc: 0.8271, Val Loss: 0.6902, Val Acc: 0.7000\n","Epoch 0320, Train Loss: 0.4452, Train Acc: 0.8346, Val Loss: 0.6956, Val Acc: 0.7000\n","Epoch 0340, Train Loss: 0.4368, Train Acc: 0.8459, Val Loss: 0.6760, Val Acc: 0.7000\n","Epoch 0360, Train Loss: 0.4198, Train Acc: 0.8496, Val Loss: 0.6624, Val Acc: 0.7000\n","Epoch 0380, Train Loss: 0.4089, Train Acc: 0.8496, Val Loss: 0.6443, Val Acc: 0.7000\n","Epoch 0400, Train Loss: 0.3821, Train Acc: 0.8346, Val Loss: 0.6611, Val Acc: 0.7000\n","Epoch 0420, Train Loss: 0.3938, Train Acc: 0.8647, Val Loss: 0.6300, Val Acc: 0.7000\n","Epoch 0440, Train Loss: 0.3456, Train Acc: 0.8459, Val Loss: 0.6128, Val Acc: 0.7667\n","Epoch 0460, Train Loss: 0.3926, Train Acc: 0.8571, Val Loss: 0.6163, Val Acc: 0.7000\n","Epoch 0480, Train Loss: 0.4259, Train Acc: 0.8233, Val Loss: 0.6148, Val Acc: 0.8000\n","Epoch 0500, Train Loss: 0.3643, Train Acc: 0.8459, Val Loss: 0.5946, Val Acc: 0.7333\n","Epoch 0520, Train Loss: 0.3431, Train Acc: 0.8759, Val Loss: 0.5777, Val Acc: 0.8000\n","Epoch 0540, Train Loss: 0.3743, Train Acc: 0.8647, Val Loss: 0.5798, Val Acc: 0.8000\n","Epoch 0560, Train Loss: 0.3641, Train Acc: 0.8684, Val Loss: 0.6092, Val Acc: 0.7000\n","Epoch 0580, Train Loss: 0.3416, Train Acc: 0.8684, Val Loss: 0.6366, Val Acc: 0.7000\n","Epoch 0600, Train Loss: 0.3104, Train Acc: 0.8684, Val Loss: 0.5968, Val Acc: 0.7667\n","Epoch 0620, Train Loss: 0.3044, Train Acc: 0.9060, Val Loss: 0.5668, Val Acc: 0.7333\n","Epoch 0640, Train Loss: 0.3299, Train Acc: 0.8722, Val Loss: 0.5789, Val Acc: 0.7667\n","Epoch 0660, Train Loss: 0.2899, Train Acc: 0.8759, Val Loss: 0.5448, Val Acc: 0.8000\n","Epoch 0680, Train Loss: 0.3079, Train Acc: 0.8797, Val Loss: 0.5477, Val Acc: 0.8000\n","Epoch 0700, Train Loss: 0.3640, Train Acc: 0.8647, Val Loss: 0.5528, Val Acc: 0.8000\n","Epoch 0720, Train Loss: 0.3706, Train Acc: 0.8647, Val Loss: 0.5186, Val Acc: 0.8000\n","Epoch 0740, Train Loss: 0.3033, Train Acc: 0.8910, Val Loss: 0.4818, Val Acc: 0.8000\n","Epoch 0760, Train Loss: 0.3486, Train Acc: 0.8759, Val Loss: 0.5203, Val Acc: 0.8000\n","Epoch 0780, Train Loss: 0.2839, Train Acc: 0.9098, Val Loss: 0.4921, Val Acc: 0.8000\n","Epoch 0800, Train Loss: 0.2492, Train Acc: 0.8872, Val Loss: 0.5114, Val Acc: 0.8000\n","Epoch 0820, Train Loss: 0.2860, Train Acc: 0.9060, Val Loss: 0.4691, Val Acc: 0.8000\n","Epoch 0840, Train Loss: 0.2802, Train Acc: 0.8947, Val Loss: 0.4554, Val Acc: 0.8000\n","Epoch 0860, Train Loss: 0.3091, Train Acc: 0.8759, Val Loss: 0.5150, Val Acc: 0.8000\n","Epoch 0880, Train Loss: 0.2419, Train Acc: 0.9173, Val Loss: 0.4926, Val Acc: 0.7667\n","Epoch 0900, Train Loss: 0.2511, Train Acc: 0.9098, Val Loss: 0.4629, Val Acc: 0.8000\n","Epoch 0920, Train Loss: 0.2717, Train Acc: 0.9023, Val Loss: 0.4541, Val Acc: 0.8000\n","Epoch 0940, Train Loss: 0.2786, Train Acc: 0.9023, Val Loss: 0.4937, Val Acc: 0.8000\n","Epoch 0960, Train Loss: 0.2939, Train Acc: 0.8872, Val Loss: 0.4804, Val Acc: 0.7667\n","Epoch 0980, Train Loss: 0.2296, Train Acc: 0.9173, Val Loss: 0.4810, Val Acc: 0.7667\n","Epoch 1000, Train Loss: 0.2388, Train Acc: 0.8947, Val Loss: 0.4779, Val Acc: 0.8000\n","Test Loss: 0.7406, Test Accuracy: 0.7973\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/maximum_spanning_tree.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17222,"status":"ok","timestamp":1755135696195,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"vTemnOfv-kR0","outputId":"33780c7d-2631-4079-e7ca-3df03ed23534"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 0.9186, Train Acc: 0.5926, Val Loss: 1.5176, Val Acc: 0.6923\n","Epoch 0040, Train Loss: 0.7458, Train Acc: 0.6667, Val Loss: 1.3441, Val Acc: 0.6923\n","Epoch 0060, Train Loss: 0.4658, Train Acc: 0.8333, Val Loss: 1.3210, Val Acc: 0.6923\n","Epoch 0080, Train Loss: 0.3702, Train Acc: 0.8704, Val Loss: 1.2757, Val Acc: 0.7692\n","Epoch 0100, Train Loss: 0.2505, Train Acc: 0.9259, Val Loss: 1.2269, Val Acc: 0.7692\n","Epoch 0120, Train Loss: 0.2656, Train Acc: 0.8981, Val Loss: 1.2480, Val Acc: 0.7692\n","Epoch 0140, Train Loss: 0.3486, Train Acc: 0.8704, Val Loss: 1.3025, Val Acc: 0.7692\n","Epoch 0160, Train Loss: 0.1573, Train Acc: 0.9444, Val Loss: 1.3845, Val Acc: 0.7692\n","Epoch 0180, Train Loss: 0.2029, Train Acc: 0.9074, Val Loss: 1.3929, Val Acc: 0.7692\n","Epoch 0200, Train Loss: 0.2101, Train Acc: 0.9352, Val Loss: 1.3231, Val Acc: 0.7692\n","Epoch 0220, Train Loss: 0.1751, Train Acc: 0.9444, Val Loss: 1.5410, Val Acc: 0.7692\n","Epoch 0240, Train Loss: 0.1773, Train Acc: 0.9259, Val Loss: 1.4123, Val Acc: 0.7692\n","Epoch 0260, Train Loss: 0.1503, Train Acc: 0.9537, Val Loss: 1.4312, Val Acc: 0.8462\n","Epoch 0280, Train Loss: 0.1478, Train Acc: 0.9352, Val Loss: 1.4694, Val Acc: 0.8462\n","Epoch 0300, Train Loss: 0.0961, Train Acc: 0.9537, Val Loss: 1.3328, Val Acc: 0.7692\n","Epoch 0320, Train Loss: 0.0871, Train Acc: 0.9815, Val Loss: 1.5917, Val Acc: 0.7692\n","Epoch 0340, Train Loss: 0.1102, Train Acc: 0.9537, Val Loss: 1.3756, Val Acc: 0.7692\n","Epoch 0360, Train Loss: 0.0728, Train Acc: 0.9630, Val Loss: 1.8553, Val Acc: 0.7692\n","Epoch 0380, Train Loss: 0.1041, Train Acc: 0.9444, Val Loss: 1.7168, Val Acc: 0.7692\n","Epoch 0400, Train Loss: 0.0807, Train Acc: 0.9722, Val Loss: 1.6941, Val Acc: 0.7692\n","Epoch 0420, Train Loss: 0.1194, Train Acc: 0.9722, Val Loss: 1.7543, Val Acc: 0.7692\n","Epoch 0440, Train Loss: 0.0443, Train Acc: 0.9907, Val Loss: 1.6086, Val Acc: 0.8462\n","Epoch 0460, Train Loss: 0.1040, Train Acc: 0.9630, Val Loss: 1.7349, Val Acc: 0.7692\n","Epoch 0480, Train Loss: 0.1229, Train Acc: 0.9630, Val Loss: 1.5068, Val Acc: 0.7692\n","Epoch 0500, Train Loss: 0.0806, Train Acc: 0.9815, Val Loss: 1.7636, Val Acc: 0.7692\n","Epoch 0520, Train Loss: 0.0561, Train Acc: 0.9815, Val Loss: 1.4343, Val Acc: 0.7692\n","Epoch 0540, Train Loss: 0.0841, Train Acc: 0.9722, Val Loss: 1.8601, Val Acc: 0.7692\n","Epoch 0560, Train Loss: 0.0686, Train Acc: 0.9907, Val Loss: 1.8437, Val Acc: 0.7692\n","Epoch 0580, Train Loss: 0.1015, Train Acc: 0.9444, Val Loss: 1.9478, Val Acc: 0.8462\n","Epoch 0600, Train Loss: 0.0694, Train Acc: 0.9815, Val Loss: 2.1560, Val Acc: 0.7692\n","Epoch 0620, Train Loss: 0.0760, Train Acc: 0.9537, Val Loss: 1.6675, Val Acc: 0.7692\n","Epoch 0640, Train Loss: 0.0483, Train Acc: 0.9907, Val Loss: 1.6830, Val Acc: 0.8462\n","Epoch 0660, Train Loss: 0.0801, Train Acc: 0.9630, Val Loss: 1.6048, Val Acc: 0.7692\n","Epoch 0680, Train Loss: 0.0525, Train Acc: 0.9907, Val Loss: 1.5028, Val Acc: 0.7692\n","Epoch 0700, Train Loss: 0.0866, Train Acc: 0.9630, Val Loss: 1.7400, Val Acc: 0.8462\n","Epoch 0720, Train Loss: 0.0825, Train Acc: 0.9630, Val Loss: 1.5597, Val Acc: 0.7692\n","Epoch 0740, Train Loss: 0.1019, Train Acc: 0.9444, Val Loss: 1.4150, Val Acc: 0.8462\n","Epoch 0760, Train Loss: 0.0739, Train Acc: 0.9722, Val Loss: 1.5607, Val Acc: 0.7692\n","Epoch 0780, Train Loss: 0.0796, Train Acc: 0.9815, Val Loss: 1.6305, Val Acc: 0.9231\n","Epoch 0800, Train Loss: 0.1385, Train Acc: 0.9722, Val Loss: 1.8214, Val Acc: 0.7692\n","Epoch 0820, Train Loss: 0.0648, Train Acc: 0.9630, Val Loss: 1.7031, Val Acc: 0.7692\n","Epoch 0840, Train Loss: 0.0515, Train Acc: 0.9722, Val Loss: 1.6925, Val Acc: 0.8462\n","Epoch 0860, Train Loss: 0.0598, Train Acc: 0.9815, Val Loss: 1.6502, Val Acc: 0.7692\n","Epoch 0880, Train Loss: 0.0872, Train Acc: 0.9444, Val Loss: 1.7368, Val Acc: 0.7692\n","Epoch 0900, Train Loss: 0.0916, Train Acc: 0.9537, Val Loss: 1.7463, Val Acc: 0.8462\n","Epoch 0920, Train Loss: 0.0969, Train Acc: 0.9630, Val Loss: 1.3118, Val Acc: 0.8462\n","Epoch 0940, Train Loss: 0.0379, Train Acc: 0.9815, Val Loss: 1.3892, Val Acc: 0.8462\n","Epoch 0960, Train Loss: 0.0559, Train Acc: 0.9815, Val Loss: 1.5031, Val Acc: 0.7692\n","Epoch 0980, Train Loss: 0.0631, Train Acc: 0.9815, Val Loss: 1.1726, Val Acc: 0.8462\n","Epoch 1000, Train Loss: 0.0393, Train Acc: 0.9907, Val Loss: 1.5974, Val Acc: 0.7692\n","Test Loss: 1.0147, Test Accuracy: 0.8065\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/global_threshold.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42963,"status":"ok","timestamp":1755135762040,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"_fdzrKEwGrzi","outputId":"264e901e-2403-42a8-93b5-a61413de3262"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 1.0739, Train Acc: 0.5985, Val Loss: 1.2947, Val Acc: 0.5161\n","Epoch 0040, Train Loss: 0.8904, Train Acc: 0.6423, Val Loss: 1.0870, Val Acc: 0.5806\n","Epoch 0060, Train Loss: 0.8025, Train Acc: 0.6861, Val Loss: 0.9332, Val Acc: 0.6774\n","Epoch 0080, Train Loss: 0.6948, Train Acc: 0.7153, Val Loss: 0.9597, Val Acc: 0.7742\n","Epoch 0100, Train Loss: 0.6014, Train Acc: 0.7701, Val Loss: 0.9366, Val Acc: 0.8387\n","Epoch 0120, Train Loss: 0.5577, Train Acc: 0.7810, Val Loss: 0.8504, Val Acc: 0.7419\n","Epoch 0140, Train Loss: 0.4837, Train Acc: 0.8321, Val Loss: 0.8930, Val Acc: 0.7097\n","Epoch 0160, Train Loss: 0.4738, Train Acc: 0.8066, Val Loss: 0.8258, Val Acc: 0.7097\n","Epoch 0180, Train Loss: 0.4684, Train Acc: 0.8102, Val Loss: 0.8238, Val Acc: 0.8387\n","Epoch 0200, Train Loss: 0.3861, Train Acc: 0.8577, Val Loss: 0.7510, Val Acc: 0.7419\n","Epoch 0220, Train Loss: 0.3859, Train Acc: 0.8613, Val Loss: 0.7559, Val Acc: 0.7097\n","Epoch 0240, Train Loss: 0.3473, Train Acc: 0.8869, Val Loss: 0.8271, Val Acc: 0.7419\n","Epoch 0260, Train Loss: 0.3006, Train Acc: 0.8832, Val Loss: 0.7761, Val Acc: 0.8387\n","Epoch 0280, Train Loss: 0.3157, Train Acc: 0.8796, Val Loss: 0.8145, Val Acc: 0.8387\n","Epoch 0300, Train Loss: 0.3142, Train Acc: 0.8504, Val Loss: 0.7775, Val Acc: 0.7097\n","Epoch 0320, Train Loss: 0.2843, Train Acc: 0.8978, Val Loss: 0.7725, Val Acc: 0.7742\n","Epoch 0340, Train Loss: 0.2932, Train Acc: 0.8942, Val Loss: 0.7947, Val Acc: 0.7419\n","Epoch 0360, Train Loss: 0.2656, Train Acc: 0.9307, Val Loss: 0.7668, Val Acc: 0.7742\n","Epoch 0380, Train Loss: 0.2628, Train Acc: 0.9161, Val Loss: 0.8104, Val Acc: 0.7419\n","Epoch 0400, Train Loss: 0.2690, Train Acc: 0.9015, Val Loss: 0.8594, Val Acc: 0.8065\n","Epoch 0420, Train Loss: 0.2940, Train Acc: 0.8869, Val Loss: 0.8403, Val Acc: 0.7742\n","Epoch 0440, Train Loss: 0.1985, Train Acc: 0.9343, Val Loss: 0.8125, Val Acc: 0.8065\n","Epoch 0460, Train Loss: 0.2325, Train Acc: 0.9197, Val Loss: 0.7584, Val Acc: 0.8387\n","Epoch 0480, Train Loss: 0.2170, Train Acc: 0.9124, Val Loss: 0.8417, Val Acc: 0.7742\n","Epoch 0500, Train Loss: 0.2941, Train Acc: 0.8650, Val Loss: 0.8327, Val Acc: 0.7419\n","Epoch 0520, Train Loss: 0.2103, Train Acc: 0.9161, Val Loss: 0.7563, Val Acc: 0.8065\n","Epoch 0540, Train Loss: 0.2279, Train Acc: 0.8978, Val Loss: 0.7778, Val Acc: 0.7419\n","Epoch 0560, Train Loss: 0.1997, Train Acc: 0.9234, Val Loss: 0.7883, Val Acc: 0.8065\n","Epoch 0580, Train Loss: 0.2310, Train Acc: 0.9343, Val Loss: 0.8795, Val Acc: 0.8387\n","Epoch 0600, Train Loss: 0.2108, Train Acc: 0.9124, Val Loss: 0.7973, Val Acc: 0.8065\n","Epoch 0620, Train Loss: 0.1918, Train Acc: 0.9343, Val Loss: 0.7679, Val Acc: 0.8065\n","Epoch 0640, Train Loss: 0.1983, Train Acc: 0.9270, Val Loss: 0.8154, Val Acc: 0.7742\n","Epoch 0660, Train Loss: 0.2239, Train Acc: 0.9307, Val Loss: 0.8102, Val Acc: 0.7742\n","Epoch 0680, Train Loss: 0.1705, Train Acc: 0.9416, Val Loss: 0.7022, Val Acc: 0.8387\n","Epoch 0700, Train Loss: 0.2584, Train Acc: 0.9088, Val Loss: 0.7953, Val Acc: 0.8065\n","Epoch 0720, Train Loss: 0.1631, Train Acc: 0.9343, Val Loss: 0.8783, Val Acc: 0.7742\n","Epoch 0740, Train Loss: 0.1846, Train Acc: 0.9234, Val Loss: 0.7287, Val Acc: 0.7742\n","Epoch 0760, Train Loss: 0.1532, Train Acc: 0.9416, Val Loss: 0.8973, Val Acc: 0.7742\n","Epoch 0780, Train Loss: 0.2030, Train Acc: 0.9124, Val Loss: 0.7698, Val Acc: 0.7097\n","Epoch 0800, Train Loss: 0.2384, Train Acc: 0.8905, Val Loss: 0.7156, Val Acc: 0.8065\n","Epoch 0820, Train Loss: 0.1795, Train Acc: 0.9489, Val Loss: 0.8320, Val Acc: 0.7419\n","Epoch 0840, Train Loss: 0.1967, Train Acc: 0.9270, Val Loss: 0.7996, Val Acc: 0.8387\n","Epoch 0860, Train Loss: 0.1688, Train Acc: 0.9562, Val Loss: 0.7183, Val Acc: 0.7419\n","Epoch 0880, Train Loss: 0.1471, Train Acc: 0.9526, Val Loss: 0.7141, Val Acc: 0.7742\n","Epoch 0900, Train Loss: 0.2106, Train Acc: 0.9197, Val Loss: 0.7523, Val Acc: 0.7097\n","Epoch 0920, Train Loss: 0.1847, Train Acc: 0.9380, Val Loss: 0.7886, Val Acc: 0.8065\n","Epoch 0940, Train Loss: 0.1547, Train Acc: 0.9526, Val Loss: 0.5622, Val Acc: 0.8065\n","Epoch 0960, Train Loss: 0.1878, Train Acc: 0.9526, Val Loss: 0.7113, Val Acc: 0.8065\n","Epoch 0980, Train Loss: 0.1341, Train Acc: 0.9526, Val Loss: 0.7383, Val Acc: 0.7742\n","Epoch 1000, Train Loss: 0.1462, Train Acc: 0.9526, Val Loss: 0.7684, Val Acc: 0.8387\n","Test Loss: 1.1184, Test Accuracy: 0.7792\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/locally_adaptive.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21961,"status":"ok","timestamp":1755135799997,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"x7FKbZLrHSUM","outputId":"6e62eec5-98b9-48a0-8228-7a4cb6c151a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 1.0777, Train Acc: 0.6316, Val Loss: 1.2738, Val Acc: 0.5000\n","Epoch 0040, Train Loss: 0.9386, Train Acc: 0.7105, Val Loss: 1.0399, Val Acc: 0.5333\n","Epoch 0060, Train Loss: 0.6887, Train Acc: 0.7444, Val Loss: 0.8901, Val Acc: 0.6667\n","Epoch 0080, Train Loss: 0.6674, Train Acc: 0.7782, Val Loss: 0.8616, Val Acc: 0.6667\n","Epoch 0100, Train Loss: 0.5982, Train Acc: 0.7857, Val Loss: 0.8111, Val Acc: 0.6667\n","Epoch 0120, Train Loss: 0.5678, Train Acc: 0.7744, Val Loss: 0.8146, Val Acc: 0.7000\n","Epoch 0140, Train Loss: 0.5667, Train Acc: 0.8308, Val Loss: 0.7883, Val Acc: 0.7000\n","Epoch 0160, Train Loss: 0.4719, Train Acc: 0.8120, Val Loss: 0.7745, Val Acc: 0.7333\n","Epoch 0180, Train Loss: 0.5247, Train Acc: 0.8008, Val Loss: 0.7573, Val Acc: 0.7333\n","Epoch 0200, Train Loss: 0.4418, Train Acc: 0.8195, Val Loss: 0.7502, Val Acc: 0.7000\n","Epoch 0220, Train Loss: 0.4166, Train Acc: 0.8346, Val Loss: 0.7294, Val Acc: 0.7333\n","Epoch 0240, Train Loss: 0.4345, Train Acc: 0.8383, Val Loss: 0.7841, Val Acc: 0.7000\n","Epoch 0260, Train Loss: 0.4325, Train Acc: 0.8421, Val Loss: 0.8212, Val Acc: 0.7000\n","Epoch 0280, Train Loss: 0.3464, Train Acc: 0.8722, Val Loss: 0.8028, Val Acc: 0.6667\n","Epoch 0300, Train Loss: 0.3020, Train Acc: 0.8797, Val Loss: 0.7708, Val Acc: 0.6667\n","Epoch 0320, Train Loss: 0.3547, Train Acc: 0.8534, Val Loss: 0.7262, Val Acc: 0.7000\n","Epoch 0340, Train Loss: 0.3311, Train Acc: 0.8684, Val Loss: 0.7834, Val Acc: 0.6333\n","Epoch 0360, Train Loss: 0.3141, Train Acc: 0.8797, Val Loss: 0.7712, Val Acc: 0.7000\n","Epoch 0380, Train Loss: 0.3834, Train Acc: 0.8722, Val Loss: 0.7180, Val Acc: 0.7333\n","Epoch 0400, Train Loss: 0.3043, Train Acc: 0.8872, Val Loss: 0.7995, Val Acc: 0.6667\n","Epoch 0420, Train Loss: 0.2813, Train Acc: 0.8797, Val Loss: 0.7818, Val Acc: 0.7667\n","Epoch 0440, Train Loss: 0.3286, Train Acc: 0.8797, Val Loss: 0.7861, Val Acc: 0.8000\n","Epoch 0460, Train Loss: 0.3040, Train Acc: 0.8835, Val Loss: 0.8696, Val Acc: 0.6667\n","Epoch 0480, Train Loss: 0.3938, Train Acc: 0.8571, Val Loss: 0.8184, Val Acc: 0.7333\n","Epoch 0500, Train Loss: 0.2869, Train Acc: 0.8759, Val Loss: 0.8629, Val Acc: 0.7333\n","Epoch 0520, Train Loss: 0.3293, Train Acc: 0.8835, Val Loss: 0.8666, Val Acc: 0.7667\n","Epoch 0540, Train Loss: 0.2740, Train Acc: 0.8872, Val Loss: 0.8621, Val Acc: 0.7667\n","Epoch 0560, Train Loss: 0.2350, Train Acc: 0.8947, Val Loss: 0.8182, Val Acc: 0.8000\n","Epoch 0580, Train Loss: 0.2676, Train Acc: 0.8947, Val Loss: 0.7744, Val Acc: 0.7667\n","Epoch 0600, Train Loss: 0.2302, Train Acc: 0.9060, Val Loss: 0.8861, Val Acc: 0.7333\n","Epoch 0620, Train Loss: 0.2414, Train Acc: 0.9060, Val Loss: 0.8366, Val Acc: 0.8000\n","Epoch 0640, Train Loss: 0.2334, Train Acc: 0.9211, Val Loss: 0.8569, Val Acc: 0.7667\n","Epoch 0660, Train Loss: 0.2522, Train Acc: 0.9023, Val Loss: 0.8251, Val Acc: 0.7667\n","Epoch 0680, Train Loss: 0.2487, Train Acc: 0.9098, Val Loss: 0.8446, Val Acc: 0.7667\n","Epoch 0700, Train Loss: 0.2249, Train Acc: 0.8947, Val Loss: 0.7998, Val Acc: 0.7333\n","Epoch 0720, Train Loss: 0.2094, Train Acc: 0.8985, Val Loss: 0.9200, Val Acc: 0.7000\n","Epoch 0740, Train Loss: 0.2428, Train Acc: 0.9098, Val Loss: 0.8244, Val Acc: 0.7333\n","Epoch 0760, Train Loss: 0.2110, Train Acc: 0.9060, Val Loss: 0.9778, Val Acc: 0.7000\n","Epoch 0780, Train Loss: 0.1966, Train Acc: 0.9248, Val Loss: 0.8331, Val Acc: 0.7667\n","Epoch 0800, Train Loss: 0.2436, Train Acc: 0.9023, Val Loss: 0.8356, Val Acc: 0.7000\n","Epoch 0820, Train Loss: 0.2255, Train Acc: 0.9060, Val Loss: 0.8030, Val Acc: 0.7333\n","Epoch 0840, Train Loss: 0.2363, Train Acc: 0.9060, Val Loss: 0.7902, Val Acc: 0.7667\n","Epoch 0860, Train Loss: 0.2080, Train Acc: 0.9173, Val Loss: 0.8966, Val Acc: 0.7333\n","Epoch 0880, Train Loss: 0.2345, Train Acc: 0.9173, Val Loss: 0.7461, Val Acc: 0.7667\n","Epoch 0900, Train Loss: 0.2466, Train Acc: 0.9060, Val Loss: 0.8214, Val Acc: 0.7000\n","Epoch 0920, Train Loss: 0.2258, Train Acc: 0.9060, Val Loss: 0.8810, Val Acc: 0.7000\n","Epoch 0940, Train Loss: 0.2257, Train Acc: 0.9023, Val Loss: 0.8473, Val Acc: 0.7333\n","Epoch 0960, Train Loss: 0.1897, Train Acc: 0.9398, Val Loss: 0.9123, Val Acc: 0.7000\n","Epoch 0980, Train Loss: 0.2100, Train Acc: 0.9248, Val Loss: 0.9350, Val Acc: 0.7000\n","Epoch 1000, Train Loss: 0.2005, Train Acc: 0.9211, Val Loss: 0.8448, Val Acc: 0.6667\n","Test Loss: 0.4481, Test Accuracy: 0.8514\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/ultrametric_distance_backbone.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28027,"status":"ok","timestamp":1755135841307,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"pCwJXXekHy7R","outputId":"8ba2b7a7-7e5b-4203-f769-c5ffe7e884a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 1.3615, Train Acc: 0.4135, Val Loss: 1.4504, Val Acc: 0.3667\n","Epoch 0040, Train Loss: 1.0198, Train Acc: 0.6128, Val Loss: 1.3555, Val Acc: 0.4000\n","Epoch 0060, Train Loss: 0.8583, Train Acc: 0.6842, Val Loss: 1.1690, Val Acc: 0.5667\n","Epoch 0080, Train Loss: 0.7744, Train Acc: 0.6842, Val Loss: 1.2053, Val Acc: 0.4667\n","Epoch 0100, Train Loss: 0.6281, Train Acc: 0.7932, Val Loss: 1.1202, Val Acc: 0.6667\n","Epoch 0120, Train Loss: 0.6641, Train Acc: 0.7105, Val Loss: 1.2872, Val Acc: 0.6667\n","Epoch 0140, Train Loss: 0.6071, Train Acc: 0.7556, Val Loss: 1.1896, Val Acc: 0.6667\n","Epoch 0160, Train Loss: 0.4269, Train Acc: 0.8722, Val Loss: 1.1810, Val Acc: 0.7000\n","Epoch 0180, Train Loss: 0.4617, Train Acc: 0.8271, Val Loss: 1.0647, Val Acc: 0.8000\n","Epoch 0200, Train Loss: 0.4447, Train Acc: 0.8421, Val Loss: 1.1114, Val Acc: 0.7667\n","Epoch 0220, Train Loss: 0.3910, Train Acc: 0.8797, Val Loss: 1.0789, Val Acc: 0.7333\n","Epoch 0240, Train Loss: 0.3420, Train Acc: 0.8684, Val Loss: 1.1535, Val Acc: 0.7333\n","Epoch 0260, Train Loss: 0.2943, Train Acc: 0.8947, Val Loss: 1.0791, Val Acc: 0.7667\n","Epoch 0280, Train Loss: 0.3147, Train Acc: 0.8647, Val Loss: 1.1611, Val Acc: 0.7333\n","Epoch 0300, Train Loss: 0.3365, Train Acc: 0.8797, Val Loss: 1.2210, Val Acc: 0.7333\n","Epoch 0320, Train Loss: 0.2464, Train Acc: 0.9211, Val Loss: 1.1372, Val Acc: 0.6667\n","Epoch 0340, Train Loss: 0.2577, Train Acc: 0.8947, Val Loss: 0.9850, Val Acc: 0.7667\n","Epoch 0360, Train Loss: 0.2536, Train Acc: 0.9248, Val Loss: 1.3643, Val Acc: 0.7000\n","Epoch 0380, Train Loss: 0.2082, Train Acc: 0.9286, Val Loss: 1.3298, Val Acc: 0.7000\n","Epoch 0400, Train Loss: 0.2634, Train Acc: 0.9135, Val Loss: 1.2875, Val Acc: 0.7000\n","Epoch 0420, Train Loss: 0.2137, Train Acc: 0.9135, Val Loss: 1.1489, Val Acc: 0.7333\n","Epoch 0440, Train Loss: 0.2342, Train Acc: 0.9060, Val Loss: 1.3106, Val Acc: 0.6667\n","Epoch 0460, Train Loss: 0.2045, Train Acc: 0.9211, Val Loss: 1.5798, Val Acc: 0.7333\n","Epoch 0480, Train Loss: 0.1977, Train Acc: 0.9323, Val Loss: 1.1879, Val Acc: 0.7000\n","Epoch 0500, Train Loss: 0.1687, Train Acc: 0.9436, Val Loss: 1.4062, Val Acc: 0.7333\n","Epoch 0520, Train Loss: 0.1446, Train Acc: 0.9624, Val Loss: 1.3251, Val Acc: 0.7333\n","Epoch 0540, Train Loss: 0.1886, Train Acc: 0.9361, Val Loss: 1.1720, Val Acc: 0.7333\n","Epoch 0560, Train Loss: 0.1580, Train Acc: 0.9398, Val Loss: 1.2506, Val Acc: 0.7667\n","Epoch 0580, Train Loss: 0.1333, Train Acc: 0.9624, Val Loss: 1.3835, Val Acc: 0.7333\n","Epoch 0600, Train Loss: 0.1798, Train Acc: 0.9323, Val Loss: 1.3595, Val Acc: 0.7667\n","Epoch 0620, Train Loss: 0.1323, Train Acc: 0.9624, Val Loss: 1.1250, Val Acc: 0.7000\n","Epoch 0640, Train Loss: 0.1794, Train Acc: 0.9398, Val Loss: 1.3294, Val Acc: 0.7333\n","Epoch 0660, Train Loss: 0.1477, Train Acc: 0.9699, Val Loss: 1.3779, Val Acc: 0.7333\n","Epoch 0680, Train Loss: 0.1224, Train Acc: 0.9662, Val Loss: 1.1729, Val Acc: 0.7333\n","Epoch 0700, Train Loss: 0.1825, Train Acc: 0.9323, Val Loss: 1.1044, Val Acc: 0.7333\n","Epoch 0720, Train Loss: 0.1562, Train Acc: 0.9248, Val Loss: 1.0979, Val Acc: 0.7333\n","Epoch 0740, Train Loss: 0.1348, Train Acc: 0.9549, Val Loss: 1.1601, Val Acc: 0.6333\n","Epoch 0760, Train Loss: 0.1719, Train Acc: 0.9173, Val Loss: 1.4380, Val Acc: 0.7000\n","Epoch 0780, Train Loss: 0.2258, Train Acc: 0.9474, Val Loss: 1.3260, Val Acc: 0.7333\n","Epoch 0800, Train Loss: 0.1569, Train Acc: 0.9436, Val Loss: 1.4259, Val Acc: 0.7333\n","Epoch 0820, Train Loss: 0.1619, Train Acc: 0.9474, Val Loss: 1.4270, Val Acc: 0.7333\n","Epoch 0840, Train Loss: 0.1205, Train Acc: 0.9511, Val Loss: 1.4541, Val Acc: 0.7667\n","Epoch 0860, Train Loss: 0.1403, Train Acc: 0.9511, Val Loss: 1.5356, Val Acc: 0.7667\n","Epoch 0880, Train Loss: 0.1177, Train Acc: 0.9737, Val Loss: 1.4898, Val Acc: 0.7667\n","Epoch 0900, Train Loss: 0.1582, Train Acc: 0.9549, Val Loss: 1.5062, Val Acc: 0.7000\n","Epoch 0920, Train Loss: 0.1061, Train Acc: 0.9699, Val Loss: 1.3530, Val Acc: 0.7333\n","Epoch 0940, Train Loss: 0.1369, Train Acc: 0.9398, Val Loss: 1.2686, Val Acc: 0.7000\n","Epoch 0960, Train Loss: 0.1412, Train Acc: 0.9662, Val Loss: 1.4127, Val Acc: 0.7333\n","Epoch 0980, Train Loss: 0.0927, Train Acc: 0.9774, Val Loss: 1.4347, Val Acc: 0.7000\n","Epoch 1000, Train Loss: 0.1629, Train Acc: 0.9474, Val Loss: 1.5524, Val Acc: 0.7000\n","Test Loss: 0.7521, Test Accuracy: 0.8378\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/high_salience_skeleton.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21043,"status":"ok","timestamp":1755135874101,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"},"user_tz":-480},"id":"y2xqwislINub","outputId":"8443b4f1-157f-4bf6-efd9-e3c5cbc6689f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0020, Train Loss: 0.6363, Train Acc: 0.7561, Val Loss: 1.0087, Val Acc: 0.6000\n","Epoch 0040, Train Loss: 0.3712, Train Acc: 0.8780, Val Loss: 0.5852, Val Acc: 0.8000\n","Epoch 0060, Train Loss: 0.2552, Train Acc: 0.9024, Val Loss: 0.3214, Val Acc: 0.9000\n","Epoch 0080, Train Loss: 0.1230, Train Acc: 0.9512, Val Loss: 0.1477, Val Acc: 1.0000\n","Epoch 0100, Train Loss: 0.0770, Train Acc: 0.9878, Val Loss: 0.3093, Val Acc: 0.9000\n","Epoch 0120, Train Loss: 0.0755, Train Acc: 0.9634, Val Loss: 0.1878, Val Acc: 0.9000\n","Epoch 0140, Train Loss: 0.0479, Train Acc: 0.9878, Val Loss: 0.2494, Val Acc: 0.9000\n","Epoch 0160, Train Loss: 0.0398, Train Acc: 1.0000, Val Loss: 0.2446, Val Acc: 0.9000\n","Epoch 0180, Train Loss: 0.0736, Train Acc: 0.9756, Val Loss: 0.2246, Val Acc: 0.9000\n","Epoch 0200, Train Loss: 0.0687, Train Acc: 0.9878, Val Loss: 0.2346, Val Acc: 0.9000\n","Epoch 0220, Train Loss: 0.0200, Train Acc: 1.0000, Val Loss: 0.0965, Val Acc: 1.0000\n","Epoch 0240, Train Loss: 0.0509, Train Acc: 0.9756, Val Loss: 0.0949, Val Acc: 1.0000\n","Epoch 0260, Train Loss: 0.0381, Train Acc: 0.9878, Val Loss: 0.4620, Val Acc: 0.9000\n","Epoch 0280, Train Loss: 0.0437, Train Acc: 1.0000, Val Loss: 0.2741, Val Acc: 0.9000\n","Epoch 0300, Train Loss: 0.0335, Train Acc: 0.9878, Val Loss: 0.3742, Val Acc: 0.9000\n","Epoch 0320, Train Loss: 0.0353, Train Acc: 0.9878, Val Loss: 0.0678, Val Acc: 1.0000\n","Epoch 0340, Train Loss: 0.0507, Train Acc: 0.9878, Val Loss: 0.3279, Val Acc: 0.9000\n","Epoch 0360, Train Loss: 0.0304, Train Acc: 0.9878, Val Loss: 0.2925, Val Acc: 0.9000\n","Epoch 0380, Train Loss: 0.0187, Train Acc: 1.0000, Val Loss: 0.1925, Val Acc: 0.9000\n","Epoch 0400, Train Loss: 0.0366, Train Acc: 0.9878, Val Loss: 0.0516, Val Acc: 1.0000\n","Epoch 0420, Train Loss: 0.0084, Train Acc: 1.0000, Val Loss: 0.2743, Val Acc: 0.9000\n","Epoch 0440, Train Loss: 0.0153, Train Acc: 1.0000, Val Loss: 0.4171, Val Acc: 0.9000\n","Epoch 0460, Train Loss: 0.0327, Train Acc: 0.9756, Val Loss: 0.2839, Val Acc: 0.9000\n","Epoch 0480, Train Loss: 0.0836, Train Acc: 0.9878, Val Loss: 0.2754, Val Acc: 0.9000\n","Epoch 0500, Train Loss: 0.0139, Train Acc: 1.0000, Val Loss: 0.1671, Val Acc: 0.9000\n","Epoch 0520, Train Loss: 0.0127, Train Acc: 1.0000, Val Loss: 0.1597, Val Acc: 0.9000\n","Epoch 0540, Train Loss: 0.0346, Train Acc: 0.9878, Val Loss: 0.4062, Val Acc: 0.9000\n","Epoch 0560, Train Loss: 0.0639, Train Acc: 0.9512, Val Loss: 0.1412, Val Acc: 0.9000\n","Epoch 0580, Train Loss: 0.0817, Train Acc: 0.9756, Val Loss: 0.3004, Val Acc: 0.9000\n","Epoch 0600, Train Loss: 0.0181, Train Acc: 1.0000, Val Loss: 0.4208, Val Acc: 0.9000\n","Epoch 0620, Train Loss: 0.0169, Train Acc: 1.0000, Val Loss: 0.1897, Val Acc: 0.9000\n","Epoch 0640, Train Loss: 0.0256, Train Acc: 1.0000, Val Loss: 0.0868, Val Acc: 1.0000\n","Epoch 0660, Train Loss: 0.0215, Train Acc: 1.0000, Val Loss: 0.0924, Val Acc: 1.0000\n","Epoch 0680, Train Loss: 0.0261, Train Acc: 1.0000, Val Loss: 0.0946, Val Acc: 1.0000\n","Epoch 0700, Train Loss: 0.0090, Train Acc: 1.0000, Val Loss: 0.5024, Val Acc: 0.9000\n","Epoch 0720, Train Loss: 0.0205, Train Acc: 1.0000, Val Loss: 0.1530, Val Acc: 0.9000\n","Epoch 0740, Train Loss: 0.0081, Train Acc: 1.0000, Val Loss: 0.0586, Val Acc: 1.0000\n","Epoch 0760, Train Loss: 0.0212, Train Acc: 1.0000, Val Loss: 0.0830, Val Acc: 1.0000\n","Epoch 0780, Train Loss: 0.0374, Train Acc: 0.9878, Val Loss: 0.2483, Val Acc: 0.9000\n","Epoch 0800, Train Loss: 0.0510, Train Acc: 0.9756, Val Loss: 0.1240, Val Acc: 0.9000\n","Epoch 0820, Train Loss: 0.0100, Train Acc: 1.0000, Val Loss: 0.1132, Val Acc: 1.0000\n","Epoch 0840, Train Loss: 0.0074, Train Acc: 1.0000, Val Loss: 0.0826, Val Acc: 1.0000\n","Epoch 0860, Train Loss: 0.0041, Train Acc: 1.0000, Val Loss: 0.3914, Val Acc: 0.9000\n","Epoch 0880, Train Loss: 0.0067, Train Acc: 1.0000, Val Loss: 0.1131, Val Acc: 1.0000\n","Epoch 0900, Train Loss: 0.0092, Train Acc: 1.0000, Val Loss: 0.2768, Val Acc: 0.9000\n","Epoch 0920, Train Loss: 0.0088, Train Acc: 1.0000, Val Loss: 0.1159, Val Acc: 1.0000\n","Epoch 0940, Train Loss: 0.0317, Train Acc: 0.9878, Val Loss: 0.0210, Val Acc: 1.0000\n","Epoch 0960, Train Loss: 0.0113, Train Acc: 1.0000, Val Loss: 0.1359, Val Acc: 0.9000\n","Epoch 0980, Train Loss: 0.0372, Train Acc: 0.9878, Val Loss: 0.0465, Val Acc: 1.0000\n","Epoch 1000, Train Loss: 0.0170, Train Acc: 0.9878, Val Loss: 0.0960, Val Acc: 1.0000\n","Test Loss: 0.5619, Test Accuracy: 0.8261\n"]}],"source":["import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- Load and Filter Data ---\n","edges_df = pd.read_csv('/content/email/modularity_backbone.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","labeled_nodes = set(comm_df['node'].tolist())\n","\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old_id: new_id for new_id, old_id in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)]\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","G = nx.from_pandas_edgelist(edges_df, 'source', 'target', edge_attr='weight')\n","num_nodes = len(G.nodes)\n","\n","# --- Feature Engineering ---\n","degree = np.array([G.degree(n) for n in range(num_nodes)])\n","clustering = np.array([nx.clustering(G, n) for n in range(num_nodes)])\n","betweenness = np.array(list(nx.betweenness_centrality(G).values()))\n","pagerank = np.array(list(nx.pagerank(G).values()))\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- Edge Index ---\n","edges = list(G.edges(data=True))\n","edge_index = torch.tensor([[e[0], e[1]] for e in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([e[2]['weight'] for e in edges], dtype=torch.float)\n","\n","# --- Label Filtering ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","for _, row in comm_df.iterrows():\n","    labels[row['node']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_k = 5\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- Model ---\n","class GraphSAGE(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels)\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.conv3 = SAGEConv(hidden_channels, out_channels)\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GraphSAGE(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","# --- Training ---\n","best_val_loss = float('inf')\n","patience = 50\n","patience_counter = 0\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    out = model(data)\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","for epoch in range(1, 1001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        patience_counter += 1\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","# --- Test Evaluation ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n","\n"]},{"cell_type":"code","source":["!pip install netbone"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaHL6PjmHE8T","executionInfo":{"status":"ok","timestamp":1755672453586,"user_tz":-480,"elapsed":13090,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"}},"outputId":"fba0f34e-0002-4490-dc6b-1e34167711f7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: netbone in /usr/local/lib/python3.12/dist-packages (0.2.3)\n","Requirement already satisfied: Cython==0.29.32 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.29.32)\n","Requirement already satisfied: igraph==0.10.2 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.10.2)\n","Requirement already satisfied: networkx==2.8.8 in /usr/local/lib/python3.12/dist-packages (from netbone) (2.8.8)\n","Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from netbone) (1.26.4)\n","Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (3.3.0)\n","Requirement already satisfied: packaging==22.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (22.0)\n","Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.12/dist-packages (from netbone) (2.2.2)\n","Requirement already satisfied: patsy==0.5.3 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.5.3)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.12/dist-packages (from netbone) (2.8.2)\n","Requirement already satisfied: pytz==2022.7 in /usr/local/lib/python3.12/dist-packages (from netbone) (2022.7)\n","Requirement already satisfied: scipy==1.13.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (1.13.0)\n","Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (1.16.0)\n","Requirement already satisfied: statsmodels==0.14.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.14.0)\n","Requirement already satisfied: tabulate==0.8.9 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.8.9)\n","Requirement already satisfied: texttable==1.6.7 in /usr/local/lib/python3.12/dist-packages (from netbone) (1.6.7)\n","Requirement already satisfied: python-louvain==0.16 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.16)\n","Requirement already satisfied: matplotlib==3.6.0 in /usr/local/lib/python3.12/dist-packages (from netbone) (3.6.0)\n","Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.13.2)\n","Requirement already satisfied: jax==0.4.23 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.4.23)\n","Requirement already satisfied: jaxlib==0.4.23 in /usr/local/lib/python3.12/dist-packages (from netbone) (0.4.23)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from jax==0.4.23->netbone) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (1.4.9)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.6.0->netbone) (3.2.3)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.2->netbone) (2025.2)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import networkx as nx\n","import netbone as nb\n","from netbone.filters import threshold_filter, boolean_filter\n","\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import SAGEConv, BatchNorm\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# --- 1. Load raw data ---\n","edges_df = pd.read_csv('/content/weighted_directed_graph.csv')\n","comm_df = pd.read_csv('/content/email-Eu-core-department-labels.txt', sep=' ', header=None, names=['node', 'label'])\n","\n","# Filter labeled nodes\n","labeled_nodes = set(comm_df['node'].tolist())\n","edges_df = edges_df[edges_df['source'].isin(labeled_nodes) & edges_df['target'].isin(labeled_nodes)]\n","\n","# Remap node IDs\n","all_nodes = sorted(set(edges_df['source']) | set(edges_df['target']))\n","node_id_map = {old: new for new, old in enumerate(all_nodes)}\n","edges_df['source'] = edges_df['source'].map(node_id_map)\n","edges_df['target'] = edges_df['target'].map(node_id_map)\n","comm_df = comm_df[comm_df['node'].isin(node_id_map)].copy()\n","comm_df['node'] = comm_df['node'].map(node_id_map)\n","\n","# --- 2. Build directed graph ---\n","G = nx.DiGraph()\n","for _, row in edges_df.iterrows():\n","    G.add_edge(row['source'], row['target'], weight=row['weight'])\n","\n","# --- 3. Extract backbone ---\n","b = nb.high_salience_skeleton(G)\n","G_back = nx.to_networkx_graph(threshold_filter(b, 0.3))\n","\n","# --- 4. Remap backbone nodes ---\n","nodes_backbone = list(G_back.nodes())\n","node_id_map_back = {old: new for new, old in enumerate(nodes_backbone)}\n","num_nodes = len(nodes_backbone)\n","\n","# --- 5. Rebuild edge list ---\n","edges = [(node_id_map_back[u], node_id_map_back[v], attr) for u, v, attr in G_back.edges(data=True)]\n","edge_index = torch.tensor([[u, v] for u, v, _ in edges], dtype=torch.long).t().contiguous()\n","edge_weight = torch.tensor([attr.get('weight', 1.0) for _, _, attr in edges], dtype=torch.float)\n","\n","# --- 6. Feature extraction ---\n","degree = np.array([G_back.degree(n) for n in nodes_backbone])\n","clustering = np.array([nx.clustering(G_back, n) for n in nodes_backbone])\n","betweenness_dict = nx.betweenness_centrality(G_back)\n","pagerank_dict = nx.pagerank(G_back)\n","\n","betweenness = np.array([betweenness_dict[n] for n in nodes_backbone])\n","pagerank = np.array([pagerank_dict[n] for n in nodes_backbone])\n","\n","features = np.vstack([degree, clustering, betweenness, pagerank]).T\n","features = StandardScaler().fit_transform(features)\n","x = torch.tensor(features, dtype=torch.float)\n","\n","# --- 7. Labels ---\n","labels = torch.full((num_nodes,), -1, dtype=torch.long)\n","comm_df_back = comm_df[comm_df['node'].isin(node_id_map_back.keys())].copy()\n","comm_df_back['node_back'] = comm_df_back['node'].map(node_id_map_back)\n","for _, row in comm_df_back.iterrows():\n","    labels[row['node_back']] = row['label']\n","\n","labels_np = labels.numpy()\n","labeled_nodes_arr = (labels != -1).nonzero(as_tuple=True)[0].numpy()\n","labeled_labels = labels_np[labeled_nodes_arr]\n","\n","top_k = 5\n","unique, counts = np.unique(labeled_labels, return_counts=True)\n","top_classes = unique[np.argsort(-counts)[:top_k]]\n","\n","mask_top = np.isin(labeled_labels, top_classes)\n","valid_nodes = labeled_nodes_arr[mask_top]\n","valid_labels = labeled_labels[mask_top]\n","label_map = {old: new for new, old in enumerate(top_classes)}\n","mapped_labels = np.array([label_map[y] for y in valid_labels])\n","labels[valid_nodes] = torch.tensor(mapped_labels, dtype=torch.long)\n","\n","train_idx, test_idx = train_test_split(valid_nodes, test_size=0.2, stratify=mapped_labels, random_state=42)\n","train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=mapped_labels[np.isin(valid_nodes, train_idx)], random_state=42)\n","\n","train_mask = torch.zeros(num_nodes, dtype=torch.bool); train_mask[train_idx] = True\n","val_mask = torch.zeros(num_nodes, dtype=torch.bool); val_mask[val_idx] = True\n","test_mask = torch.zeros(num_nodes, dtype=torch.bool); test_mask[test_idx] = True\n","\n","data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=labels,\n","            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n","\n","# --- 8. Class weights ---\n","class_counts = np.array([np.sum(mapped_labels == i) for i in range(top_k)])\n","weights = 1.0 / (class_counts + 1e-6)\n","weights = weights / weights.sum()\n","class_weights = torch.tensor(weights, dtype=torch.float)\n","\n","# --- 9. Custom Backbone Pooling Layer ---\n","class BackbonePooling(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        row, col = edge_index\n","        weight_sum = torch.zeros_like(x)\n","        norm = torch.zeros(x.size(0), 1).to(x.device)\n","\n","        for i in range(edge_index.size(1)):\n","            src, dst = row[i], col[i]\n","            weight = edge_attr[i]\n","            weight_sum[dst] += weight * x[src]\n","            norm[dst] += weight\n","\n","        norm[norm == 0] = 1\n","        return weight_sum / norm\n","\n","# --- 10. GNN Model with Custom Pooling ---\n","class GraphSAGENodeClassifier(torch.nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.4):\n","        super().__init__()\n","        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='max')\n","        self.bn1 = BatchNorm(hidden_channels)\n","        self.pool = BackbonePooling()\n","        self.conv2 = SAGEConv(hidden_channels, hidden_channels, aggr='max')\n","        self.bn2 = BatchNorm(hidden_channels)\n","        self.classifier = SAGEConv(hidden_channels, out_channels, aggr='max')\n","        self.dropout = dropout\n","\n","    def forward(self, data):\n","        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n","        x = self.conv1(x, edge_index)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.pool(x, edge_index, edge_attr)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        x = self.conv2(x, edge_index)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","\n","        x = self.classifier(x, edge_index)\n","        return x\n","\n","# --- 11. Model Training ---\n","model = GraphSAGENodeClassifier(data.num_features, 128, top_k)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n","criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Optional: DropEdge regularization (edge dropout)\n","    edge_index, edge_attr = data.edge_index.clone(), data.edge_attr.clone()\n","    keep_prob = 0.9\n","    mask = torch.rand(edge_index.size(1)) < keep_prob\n","    edge_index = edge_index[:, mask]\n","    edge_attr = edge_attr[mask]\n","\n","    out = model(Data(x=data.x, edge_index=edge_index, edge_attr=edge_attr))\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","    loss.backward()\n","    optimizer.step()\n","    pred = out.argmax(dim=1)\n","    acc = (pred[data.train_mask] == data.y[data.train_mask]).sum().item() / data.train_mask.sum().item()\n","    return loss.item(), acc\n","\n","def evaluate(mask):\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(data)\n","        loss = criterion(out[mask], data.y[mask])\n","        pred = out.argmax(dim=1)\n","        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n","    return loss.item(), acc\n","\n","# --- 12. Training loop ---\n","best_val_loss = float('inf')\n","counter = 0\n","patience = 50\n","\n","for epoch in range(1, 2001):\n","    train_loss, train_acc = train()\n","    val_loss, val_acc = evaluate(data.val_mask)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        counter = 0\n","        torch.save(model.state_dict(), 'best_model.pth')\n","    else:\n","        counter += 1\n","\n","    if epoch % 20 == 0:\n","        print(f\"Epoch {epoch:04d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n","              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n","\n","# --- 13. Final Test ---\n","model.load_state_dict(torch.load('best_model.pth'))\n","test_loss, test_acc = evaluate(data.test_mask)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"tWGwevkxeIO2","executionInfo":{"status":"error","timestamp":1755679325071,"user_tz":-480,"elapsed":4591,"user":{"displayName":"Aurora Qiu","userId":"13240399701771409106"}},"outputId":"3782fa38-af99-4115-dd19-075429c8ef80"},"execution_count":9,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1162608189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# --- 3. Extract backbone ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh_salience_skeleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mG_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_networkx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/netbone/structural/high_salience_skeleton.py\u001b[0m in \u001b[0;36mhigh_salience_skeleton\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_source_dijkstra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mnode_tree_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/algorithms/shortest_paths/weighted.py\u001b[0m in \u001b[0;36msingle_source_dijkstra\u001b[0;34m(G, source, target, cutoff, weight)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0msingle_source_bellman_ford\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \"\"\"\n\u001b[0;32m--> 472\u001b[0;31m     return multi_source_dijkstra(\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/algorithms/shortest_paths/weighted.py\u001b[0m in \u001b[0;36mmulti_source_dijkstra\u001b[0;34m(G, sources, target, cutoff, weight)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_weight_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# dictionary of paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     dist = _dijkstra_multisource(\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/algorithms/shortest_paths/weighted.py\u001b[0m in \u001b[0;36m_dijkstra_multisource\u001b[0;34m(G, sources, weight, pred, paths, cutoff, target)\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mvu_dist\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO82U07X8oYj1lHy0MZVQG1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}